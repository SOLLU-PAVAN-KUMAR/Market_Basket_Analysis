---
title: "SOLLU PAVAN_IIT-MADRAS_2"
author: "Pavan"
date: "13/06/2020"
output: html_document
---

```{r}
# Load libraries
library(tidyverse)
library(arules)
library(arulesViz)
library(knitr)
library(gridExtra)
library(lubridate)
```

```{r}
data = read_csv("C:/Users/pavan/Downloads/transaction_data.csv")
summary(data)
```
<br> The negative values in 'userId' and 'ItemCode' is the missing data where as the negative values in 'NumberofItemsPurchased' and 'CostperItem' are because of cancelled or refunded values.<br><br>
We will perform some cleaning in data.<br><br>

```{r}
glimpse(data)
```

```{r}
dim(data)
head(data)
```

```{r}
data$UserId[data$UserId < 0] = NA # cleaning the negative data
data$ItemCode[data$ItemCode < 0] = NA
```

```{r}
library(DataExplorer)
options(repr.plot.width=8, repr.plot.height=8)
plot_missing(data)
```
<br> We can see that except 'user_id' all the remaining features have almost all data available.<br><br>

We know 'user_id' is not an important feature to predict the 'items of basket' so instead of omiting with respect to 'user_id' we will go with other features, so that we can perform further analysis on almost full data set.<br><br>

```{r}
data$UserId = NULL
dim(data)
```

```{r}
options(repr.plot.width=8, repr.plot.height=8)
plot_missing(data)
```
<br> As this missing data is very small compared to data set, thus we can omit this missing row's and continue our analysis on remaining data.<br><br>

```{r}
data = na.omit(data)
dim(data)
```

```{r}
summary(data)
```


```{r}
boxplot(data$TransactionId,data$ItemCode,data$NumberOfItemsPurchased,data$CostPerItem,names = c("transactionId","Itemcode","No.of items","cost"),main = "Box plot of columns of transaction data")
```

```{r}
boxplot(data$ItemCode,data$NumberOfItemsPurchased,data$CostPerItem,names = c("Itemcode","No.of items","cost"),main = "Box plot of columns of transaction data without transaction_id")
```


```{r}
# Extracting date,time,month,day,year and adding as new columns for EDA

data$day <- sapply(data$TransactionTime, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][1]})
data$month <- sapply(data$TransactionTime, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][2]})
data$date <- sapply(data$TransactionTime, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][3]})
data$time <- sapply(data$TransactionTime, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][4]})
data$year <- sapply(data$TransactionTime, FUN = function(x) {strsplit(x, split = '[ ]')[[1]][6]})
dim(data)
head(data)
```
```{r}
data$TransactionTime = NULL
dim(data)
```

```{r}
data$hour <- sapply(data$time, FUN = function(x) {strsplit(x, split = '[:]')[[1]][1]})
```

```{r}
summary(data)
```
<br>Now type caste the date,year and hour to numeric for further analysis<br>

```{r}
data$date = as.numeric(data$date)# type casting char to numeric
data$year = as.numeric(data$year)
data$hour = as.numeric(data$hour)
summary(data)
```
<br> We can see from the above summary that the year has max value as 2028 but the data set is for year's 2018 and 2019 thus it is recognised as typing error and let's change 2028 to 2018.<br><br>
All the remaining features has in range values thus no cleaning is required for them.<br><br>
```{r}
data$year[data$year == 2028] = 2018
summary(data$year)
```

```{r}
data$bill = data$NumberOfItemsPurchased * data$CostPerItem # Extracting revenue for
head(data)                                                # revenue analysis
summary(data$bill)
```
<br>The negative value in 'bill' are because of order cancellation.<br><br>


```{r}
data$Country = as.factor(data$Country) # Extracting levels of data
data$month = as.factor(data$month)
data$year = as.factor(data$year)
levels(data$year) = c(2018,2019)
data$hour = as.factor(data$hour)
data$day = as.factor(data$day)
levels(data$month)
levels(data$Country)
levels(data$day)
```
<br> From above we can conclude that all the data is clear without any errors.<br><br>


```{r}
country_Summary = data %>%
  group_by(Country) %>%
  summarise(Number_of_Orders=n(),revenue = sum(bill), transactions = n_distinct(TransactionId)) %>%
  ungroup() %>%
  arrange(desc(revenue))

head(country_Summary)
unique(country_Summary$Country)
```
<br> As 'UK' values are more dominating we can pridict that it can be a 'UK' based company.<br>

Let's visualise the remaining top countries.<br>
```{r}
top_Countries <- data %>%
  filter(Country == 'Netherlands'| Country == 'EIRE' | Country == 'Germany' | Country == 'France' | Country == 'Australia')

top_Country_Summary <- top_Countries %>%
  group_by(Country, month) %>%
  summarise(revenue = sum(bill), transactions = n_distinct(TransactionId))%>%
  ungroup() %>%
  arrange(desc(revenue))
head(top_Country_Summary)
summary(top_Country_Summary)
```

```{r}
ggplot(top_Country_Summary, aes(x = Country, y = revenue), colour = Country) + geom_col() + labs(x = ' Country', y = 'Revenue', title = 'Revenue by Country')
ggplot(top_Country_Summary, aes(x = month, y = revenue,fill = Country))+geom_col()+ labs(x = 'month', y = 'Revenue', title = 'Revenue by Country over month')

```
<br> From the above plots it is clear that the revenue generated in each month by 'Netherland' and 'EIRE' is high compared to other contries.<br>

Thus they were on top in the list of 'revenue by country'.<br>

```{r}
orders_per_day<-data%>%group_by(day)%>%summarise(Number_of_Orders=n())%>%mutate(Percentage_of_orders=(Number_of_Orders*100/nrow(data)))

#Visualizing Number of Orders per day of the week
col = c("A","B","C","D","E","F")
df = cbind(df,col)
ggplot(orders_per_day,aes(x=day,y=Percentage_of_orders,label=paste0(round(Percentage_of_orders,1)),fill = col))+
  geom_bar(stat = "identity")+labs(title="% of Orders per day of the Week",y="Percentage of Total Orders",x="Day of the Week")+
  geom_text(nudge_y = .5)

#Visualizing revenue per day of the week
data %>%
  group_by(day) %>%
  summarise(revenue=sum(bill)) %>%
  ggplot(aes(x = day, y = revenue,fill = day)) + geom_col() + labs(x = 'Day', y = 'revenue', title = 'revenue per Day')

#Visualizing No.of transactions per day of the week
data %>%
  group_by(day) %>%
  summarise(transactions=n_distinct(TransactionId)) %>%
  ggplot(aes(x = day, y = transactions,fill = day)) + geom_col() + labs(x = 'Day', y = 'transactions', title = 'No.of transactions per Day')
```

```{r}
#Visualizing Number of Orders per hour
data %>%
  group_by(hour) %>%
  summarise(Number_of_Orders=n()) %>%
  ggplot(aes(x = hour, y = Number_of_Orders,fill = hour)) + geom_col() + labs(x = 'Hour Of Day', y = 'orders', title = 'orders by Hour Of Day')

#Visualizing revenue by hour
data %>%
  group_by(hour) %>%
  summarise(revenue=sum(bill)) %>%
  ggplot(aes(x = hour, y = revenue,fill = hour)) + geom_col() + labs(x = 'hour', y = 'revenue', title = 'revenue by hour')

#Visualizing Number of transactions by hour
data %>%
  group_by(hour) %>%
  summarise(transactions=n_distinct(TransactionId)) %>%
  ggplot(aes(x = hour, y = transactions,fill = hour)) + geom_col() + labs(x = 'hour', y = 'transactions', title = 'No.of transactions by hour')
```

```{r}
#Visualizing Number of Orders per month
data %>%
  group_by(month) %>%
  summarise(Number_of_Orders=n()) %>%
  ggplot(aes(x = month, y = Number_of_Orders,fill = month)) + geom_col() + labs(x = 'month', y = 'orders', title = 'orders per month')

#Visualizing revenue per month
data %>%
  group_by(month) %>%
  summarise(revenue=sum(bill)) %>%
  ggplot(aes(x = month, y = revenue,fill = month)) + geom_col() + labs(x = 'month', y = 'revenue', title = 'revenue per month')

#Visualizing No.of transactions per month
data %>%
  group_by(month) %>%
  summarise(transactions=n_distinct(TransactionId)) %>%
  ggplot(aes(x = month, y = transactions,fill = month)) + geom_col() + labs(x = 'month', y = 'transactions', title = 'No.of transactions per month')
```
<br>Thus so far we have performed some cleaning, Extracting some important features, and did some Exploratory data analysis (EDA), Visualisation.<br>

Now we are going to analyse the market basket history to pridict the combo's of the items
with the help of Association Rules.<br>


```{r}
# MARKET BASKET ANALYSIS (APRIORI ALGO)

transactions = split(data$ItemCode,data$TransactionId)
transactions = as(transactions,"transactions") # Creating the transactions object
transactions
```

```{r}
itemFrequencyPlot(transactions, topN = 20, type="absolute", col="peachpuff2",xlab="Item Code",ylab="absolute Frequency", main="Absolute Item Frequency Plot")
itemFrequencyPlot(transactions, topN=20, type="relative", col="DarkMagenta",xlab="Item Code",ylab="relative Frequency", main="relative Item Frequency Plot")

```

```{r}
cat("These were the items bought per transaction\n\n")
inspect(transactions[1:3])
```

<br> Now let us extract the good support values which increases the confidence and lift.<br>
```{r}
# Support and confidence values
support_Levels <- c(0.05, 0.02, 0.003, 0.001)
confidence_Levels <- c(0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)

# Empty integers 
rules_sup5 <- integer(length=9)
rules_sup2 <- integer(length=9)
rules_sup0.3 <- integer(length=9)
rules_sup0.1 <- integer(length=9)

# Apriori algorithm with a support level of 5%
for (i in 1:length(confidence_Levels)) {
  
  rules_sup5[i] <- length(apriori(transactions, parameter=list(sup=support_Levels[1], 
                                   conf=confidence_Levels[i],minlen = 3,maxlen = 3,
                                   target="rules")))
  
}

# Apriori algorithm with a support level of 2%
for (i in 1:length(confidence_Levels)){
  
  rules_sup2[i] <- length(apriori(transactions, parameter=list(sup=support_Levels[2], 
                                  conf=confidence_Levels[i],minlen = 3,maxlen = 3,
                                  target="rules")))
}

# Apriori algorithm with a support level of 0.3%
for (i in 1:length(confidence_Levels)){
  
  rules_sup0.3[i] <- length(apriori(transactions, parameter=list(sup=support_Levels[3], 
                                  conf=confidence_Levels[i],minlen = 3,maxlen = 3,
                                  target="rules")))
  
}

# Apriori algorithm with a support level of 0.1%
for (i in 1:length(confidence_Levels)){
  
  rules_sup0.1[i] <- length(apriori(transactions, parameter=list(sup=support_Levels[4], 
                                  conf=confidence_Levels[i],minlen = 3,maxlen = 3,
                                  target="rules")))
  
}

```

```{r}
# Data frame
num_rules <- data.frame(rules_sup5, rules_sup2, rules_sup0.3,rules_sup0.1, confidence_Levels)

# Number of rules found with a support level of 5%, 2%, 0.3% and 0.1%
ggplot(num_rules, aes(x=confidence_Levels)) +
  
  # Plot line and points (support level of 5%)
  geom_line(aes(y=rules_sup5, colour="5%")) + 
  geom_point(aes(y=rules_sup5, colour="5%")) +
  
  # Plot line and points (support level of 2%)
  geom_line(aes(y=rules_sup2, colour="2%")) +
  geom_point(aes(y=rules_sup2, colour="2%")) +
  
  # Plot line and points (support level of 0.3%)
  geom_line(aes(y=rules_sup0.3, colour="0.3%")) + 
  geom_point(aes(y=rules_sup0.3, colour="0.3%")) +
  
  # Plot line and points (support level of 0.1%)
  geom_line(aes(y=rules_sup0.1, colour="0.1%")) +
  geom_point(aes(y=rules_sup0.1, colour="0.1%")) +
  
  # Labs and theme
  labs(x="Confidence_levels", y="Number of rules found", 
       title="Apriori algorithm with different support levels") +
  theme_bw() +
  theme(legend.title=element_blank())

```

```{r}
ggplot(num_rules, aes(x=confidence_Levels)) +
  
  geom_line(aes(y=rules_sup0.3, colour="0.3%")) + 
  geom_point(aes(y=rules_sup0.3, colour="0.3%"))+
  geom_line(aes(y=rules_sup2, colour="2%")) + 
  geom_point(aes(y=rules_sup2, colour="2%"))+
  
  labs(x="Confidence_levels", y="Number of rules found", 
       title="Apriori algorithm with different support levels") +
  theme_bw() +
  theme(legend.title=element_blank())

```

```{r}
summary(rules_sup0.3)
summary(rules_sup0.1)
```

```{r}
boxplot(rules_sup0.3,rules_sup0.1,names = c("0.3%","0.1%"),col = c("red","light blue"), main = "box plot of rules with 0.3% and 0.1% support",xlab = "support",ylab = "No.of rules")
```
<br>As we can see the support level of 0.3% has rules far much less than 0.1% and most of rules for 0.1% support will be undesirable with low support,confidence ans lift values.<br><br> 

```{r}
boxplot(rules_sup0.3,names = c("0.3%"),col = c("tan"), main = "box plot of rules with 0.3% support",xlab = "support",ylab = "No.of rules",notch = T)

```
<br> The 0.3% support also given large number of rules but in further process as we filter the rules with some conditions we will get desirable rules.<br> 

Thus support of 0.3% is best for further analysis<br> 

Thus it is best suitable for our prediction of 2 more items.<br><br>

```{r}
rules <- apriori(transactions, parameter = list(supp = 0.003,minlen = 3,maxlen = 3))
summary(rules)
```
<br> As we can see the confidence level varies from 80% to 100% which is good prediction and also we can see the lift ranges from 7 to 128 which is far from 1.<br><br>

Thus from the confidence and the lift values we can accept these rules for predictions.<br>


```{r}
plot(rules)
```

```{r}
rules_by_lift = sort(rules,by="lift")
write(rules_by_lift,"D:\\Internship\\rules_by_lift.txt", row.names = FALSE)
inspect(rules_by_lift[1:10])
plot(head(rules_by_lift,15),method="graph")
```


```{r}
rules_by_confidence = sort(rules,by="confidence")
write(rules_by_confidence,"D:\\Internship\\rules_by_confidence.txt", row.names = FALSE)
inspect(rules_by_lift[1:10])
plot(head(rules_by_confidence,15),method="graph")
```
<br> The above graphs shows different set's of items which will be purchased in combo with greater than 80% confidence.<br>

We can also see the lift ranges from 7 to 125 which are sufficently large.<br>

Therefore we can say that by placing these products side by side in store generates more revenue.<br>
